{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9a3b92-6375-4921-b5eb-98356381b4c5",
   "metadata": {},
   "source": [
    "# Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ba1c06-0d04-485b-ba9f-91fb70f78943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd222ba3-8b3a-4e15-9240-bd2b111dae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE = 8\n",
    "NUM_SEGMENTS = 2\n",
    "RES_NEXT_OUT = 2048\n",
    "NUM_EPOCHS = 20\n",
    "CHECKPOINT_FOLDER = os.path.join('models_checkpoints', 'model_2')\n",
    "METRICS_FOLDER = os.path.join('metrics', 'model_2')\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa2cfe0-a555-4c5d-8e37-844120896e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_PATH = \"jester-v1-labels.csv\"\n",
    "TRAIN_LABELS = \"train.csv\"\n",
    "VAL_LABLES = \"val.csv\"\n",
    "TEST_LABELS = \"test.csv\"\n",
    "DATA_ROOT = \"20bn-jester-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42316bf-4004-4e1f-9519-a2be1a84575c",
   "metadata": {},
   "source": [
    "Load the pretrained ResNeXt101_32x8d model to use it for feature extraction \\\n",
    "We load it here as we get the transformation function from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0decc522-93a8-49f4-9bb1-e08a95a0e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = models.ResNeXt101_32X8D_Weights.DEFAULT\n",
    "res_next = models.resnext101_32x8d(weights=weights)\n",
    "res_next.eval()\n",
    "res_next = nn.Sequential(*list(res_next.children())[:-1])\n",
    "# Freeze all layers so that they are not updated during training\n",
    "for param in res_next.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Get the transformations needed for the model\n",
    "preprocess_transform = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2703009-36ee-49b6-a613-ab6e11d4f8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[232]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1746744-db90-4120-b760-d00b8bb2ee19",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36a268-ad5d-4b16-b54f-ac428d19dd1a",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d839161b-0236-485c-a0d4-3b4aee8d01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABELS_PATH) as labels_file:\n",
    "    labels = labels_file.readlines()\n",
    "    #labels = [label[:-1] for label in labels]\n",
    "    labels_encode_dict = dict(zip(labels, range(len(labels))))\n",
    "    labels_decode_dict = dict(zip(range(len(labels)), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1268574e-4e0d-4078-9cb8-9f4bf61cf038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, label_dict, n_segments, random, transform=None, frame_limit=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with the root directory for the videos,\n",
    "        the split (train/val/test), an optional data transformation,\n",
    "        and an optional label dictionary.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Root directory for videos\n",
    "            split (str): Split to use ('train', 'val', or 'test').\n",
    "            transform (callable, optional): Optional data transformation to apply to the images.\n",
    "            label_dict (dict, optional): Optional dictionary mapping integer labels to class names.\n",
    "        \"\"\"\n",
    "        assert split in ['train', 'val', 'test']\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.label_dict = label_dict\n",
    "        self.frame_limit = frame_limit\n",
    "        self.n_segments = n_segments\n",
    "        self.random = random\n",
    "        self.videos_paths = []\n",
    "        self.labels_num = []\n",
    "        self.labels_str = []\n",
    "\n",
    "        with open(self.split + '.csv') as r:\n",
    "            lines = r.readlines()\n",
    "            for line in lines:\n",
    "                line = line.split(';')\n",
    "                self.videos_paths.append(line[0])\n",
    "                self.labels_num.append(label_dict[line[1]])\n",
    "                self.labels_str.append(line[1])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of images in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of images in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels_num)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _select_frames(list_of_frames, num_segments, random):\n",
    "        n = len(list_of_frames)\n",
    "        segment_boundaries = np.linspace(0, n, num_segments + 1, dtype=int)  # Define segment boundaries\n",
    "        if not random:\n",
    "            selected_indices = segment_boundaries[:-1]  # Take the first index of each segment\n",
    "        else:\n",
    "            selected_indices = [np.random.randint(segment_boundaries[i], segment_boundaries[i + 1]) \n",
    "                            for i in range(num_segments)]  # Sample 1 index per segment\n",
    "        selected_frames = [list_of_frames[i] for i in selected_indices]  # Map indices to frames\n",
    "    \n",
    "        return selected_frames\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.root_dir, self.videos_paths[idx])\n",
    "        label = self.labels_num[idx]\n",
    "    \n",
    "        # Load all frames in the video\n",
    "        frame_files = sorted([f for f in os.listdir(video_path) if f.endswith(\".jpg\")])\n",
    "        if self.frame_limit:\n",
    "            frame_files = frame_files[:self.frame_limit]\n",
    "\n",
    "        if self.n_segments:\n",
    "            frame_files = self._select_frames(frame_files, self.n_segments, self.random)\n",
    "    \n",
    "        frames = []\n",
    "        for frame_file in frame_files:\n",
    "            frame_path = os.path.join(video_path, frame_file)\n",
    "            frame = Image.open(frame_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)  # Apply transform to convert to tensor\n",
    "            else:\n",
    "                frame = transforms.ToTensor()(frame)  # Default conversion if no transform provided\n",
    "            frames.append(frame)\n",
    "    \n",
    "        # Stack frames into a tensor (T x C x H x W)\n",
    "        video_tensor = torch.stack(frames)\n",
    "    \n",
    "        return video_tensor, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222121cc-acbb-43c8-b81a-c22599659a47",
   "metadata": {},
   "source": [
    "## Define datasets and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e391e0c-1bf3-4adc-ac6d-4b070bc4f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations_train = v2.Compose([\n",
    "    v2.RandomApply([v2.ElasticTransform(alpha=50.0, sigma=9.0)], p=0.2),\n",
    "    v2.ColorJitter(\n",
    "        brightness=0.1,\n",
    "        contrast=0.1,\n",
    "        saturation=0.1,\n",
    "        hue=0.1\n",
    "    ),\n",
    "    v2.RandomAdjustSharpness(sharpness_factor=2),\n",
    "    v2.RandomAutocontrast(),\n",
    "    v2.RandomEqualize(),\n",
    "    preprocess_transform\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9685df16-51a5-4a5e-82aa-019ada589c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(DATA_ROOT, \"train\", labels_encode_dict, transform=transformations_train, n_segments=NUM_SEGMENTS, random=False)\n",
    "val_dataset = VideoDataset(DATA_ROOT, \"val\", labels_encode_dict, transform=preprocess_transform, n_segments=NUM_SEGMENTS, random=False)\n",
    "test_dataset = VideoDataset(DATA_ROOT, \"test\", labels_encode_dict, transform=preprocess_transform, n_segments=NUM_SEGMENTS, random=False)\n",
    "test_dataset_rand = VideoDataset(DATA_ROOT, \"test\", labels_encode_dict, transform=preprocess_transform, n_segments=NUM_SEGMENTS, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00815ae7-a1b2-46ce-adb0-d306d5e6eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader_rand = DataLoader(\n",
    "    test_dataset_rand,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575feaf0-55c1-4f81-8e6e-f3d256716f51",
   "metadata": {},
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79ddf387-7930-42e1-82a2-2278876baf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureClassifier1(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # define \n",
    "        self.ln1 = nn.Linear(input_size, int(input_size/4))\n",
    "        self.ln2 = nn.Linear(int(input_size/4), int(input_size/8))\n",
    "        self.ln3 = nn.Linear(int(input_size/8), num_classes)\n",
    "\n",
    "        # init\n",
    "        self.initialize_layer(self.ln1)\n",
    "        self.initialize_layer(self.ln2)\n",
    "        self.initialize_layer(self.ln3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.ln1(x))\n",
    "        x = torch.relu(self.ln2(x))\n",
    "        x = self.ln3(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "class GestureClassifier2(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # define \n",
    "        self.ln1 = nn.Linear(input_size, int(input_size/1.5))\n",
    "        self.ln2 = nn.Linear(int(input_size/1.5), int(input_size/3))\n",
    "        self.ln3 = nn.Linear(int(input_size/3), int(input_size/6))\n",
    "        self.ln4 = nn.Linear(int(input_size/6), int(input_size/12))\n",
    "        self.ln5 = nn.Linear(int(input_size/12), num_classes)\n",
    "        \n",
    "        # init\n",
    "        self.initialize_layer(self.ln1)\n",
    "        self.initialize_layer(self.ln2)\n",
    "        self.initialize_layer(self.ln3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.ln1(x))\n",
    "        x = torch.relu(self.ln2(x))\n",
    "        x = torch.relu(self.ln3(x))\n",
    "        x = torch.relu(self.ln4(x))\n",
    "        x = self.ln5(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "class GestureClassifier3_6(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.bottleneck_size = 512\n",
    "\n",
    "        # define \n",
    "        self.ln1 = nn.Linear(input_size, self.bottleneck_size)\n",
    "        self.ln2 = nn.Linear(self.bottleneck_size, num_classes)\n",
    "        \n",
    "        # init\n",
    "        self.initialize_layer(self.ln1)\n",
    "        self.initialize_layer(self.ln2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(x) # as we didn't have it for the output of the ResNeXt\n",
    "        x = torch.relu(self.ln1(x))\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1c52f-171e-432d-b262-650073320882",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a1d2d8b-8b75-4adf-8858-ae618fe75d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_batch(res_next, video_batch):\n",
    "    \"\"\"\n",
    "    Process video frames and extract features using res_next.\n",
    "    Args:\n",
    "        res_next: Pretrained feature extractor (e.g., ResNeXt).\n",
    "        video_batch: Tensor of shape [batch_size, num_frames, 3, 224, 224].\n",
    "\n",
    "    Returns:\n",
    "        Concatenated features for each video: [batch_size, num_frames * feature_dim].\n",
    "    \"\"\"\n",
    "    batch_size, num_frames, c, h, w = video_batch.shape\n",
    "\n",
    "    # Reshape to process frames independently\n",
    "    frames = video_batch.view(batch_size * num_frames, c, h, w)  # [batch_size * num_frames, 3, 224, 224]\n",
    "    \n",
    "    # Extract features for each frame\n",
    "    frame_features = res_next(frames)  # Output shape: [batch_size * num_frames, feature_dim]\n",
    "    \n",
    "    # Reshape back to group frames for each video\n",
    "    frame_features = frame_features.view(batch_size, num_frames, -1)  # [batch_size, num_frames, feature_dim]\n",
    "    \n",
    "    # Concatenate features along the temporal dimension\n",
    "    fused_features = frame_features.view(batch_size, -1)  # [batch_size, num_frames * feature_dim]\n",
    "    \n",
    "    return fused_features\n",
    "\n",
    "\n",
    "def evaluate(model, features_model, eval_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the classifier on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model (CNN): CNN classifier to evaluate.\n",
    "        features_model: CNN to extract features from images. \n",
    "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
    "        criterion (callable): Loss function to use for evaluation.\n",
    "        device (torch.device): Device to use for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss on the test set.\n",
    "        float: Accuracy on the test set.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        for inputs, labels in eval_loader:\n",
    "            # Move inputs and labels to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute the logits and loss\n",
    "            logits = model(process_video_batch(features_model, inputs))\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            num_samples += len(inputs)\n",
    "\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = num_correct / num_samples\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train(model, features_model, train_loader, val_loader, optimizer, criterion, device,\n",
    "          num_epochs, with_train_set_metrics=False):\n",
    "    \"\"\"\n",
    "    Train the classifer on the training set and evaluate it on the validation set every epoch.\n",
    "\n",
    "    Args:\n",
    "    model (CNN): classifier to train.\n",
    "    features_model: CNN to extract features from images. \n",
    "    train_loader (torch.utils.data.DataLoader): Data loader for the training set.\n",
    "    val_loader (torch.utils.data.DataLoader): Data loader for the validation set.\n",
    "    optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
    "    criterion (callable): Loss function to use for training.\n",
    "    device (torch.device): Device to use for training.\n",
    "    num_epochs (int): Number of epochs to train the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Place the model on device\n",
    "    model = model.to(device)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Set model to training mode\n",
    "\n",
    "        with tqdm(total=len(train_loader),\n",
    "                  desc=f'Epoch {epoch +1}/{num_epochs}',\n",
    "                  position=0,\n",
    "                  leave=True) as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                #Move inputs and labels to device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Compute the logits and loss\n",
    "                logits = model(process_video_batch(features_model, inputs))\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "                \n",
    "            avg_loss, accuracy = evaluate(model, features_model, val_loader, criterion, device)\n",
    "            print(\n",
    "                f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}'\n",
    "                )\n",
    "            if with_train_set_metrics:\n",
    "                train_avg_loss, train_accuracy = evaluate(model, features_model, train_loader, criterion, device)\n",
    "                print (\n",
    "                    f'Train set: Average loss = {train_avg_loss:.4f}, Accuracy = {train_accuracy:.4f}'\n",
    "                )\n",
    "                losses.append((train_avg_loss, avg_loss))\n",
    "                accuracies.append((train_accuracy, accuracy))\n",
    "            else:\n",
    "                losses.append(avg_loss)\n",
    "                accuracies.append(accuracy)\n",
    "            with open(os.path.join(METRICS_FOLDER, 'losses.pkl'), 'wb') as f:\n",
    "                pickle.dump(losses, f)\n",
    "            with open(os.path.join(METRICS_FOLDER, 'accuracies.pkl'), 'wb') as f:\n",
    "                pickle.dump(accuracies, f)\n",
    "            torch.save(\n",
    "                {\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict':optimizer.state_dict()\n",
    "                },\n",
    "                os.path.join(CHECKPOINT_FOLDER, f'model_{epoch+1}_out_of_{num_epochs}.ckpt')\n",
    "            )\n",
    "\n",
    "        # plt.clf()  # Clear the current figure\n",
    "        # plt.plot(losses[:, 0], label='Training Loss')\n",
    "        # plt.plot(losses[:, 1], label='Validation Loss')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.ylabel('Loss')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        # plt.pause(0.1)  # Pause to update the plot\n",
    "        torch.save(\n",
    "            {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict()\n",
    "            }, \n",
    "            os.path.join(CHECKPOINT_FOLDER, 'model.ckpt')\n",
    "        )\n",
    "\n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf9a4e-9362-4a83-b2ff-0dc07f7572de",
   "metadata": {},
   "source": [
    "# Models - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5db3f9fc-d3a5-4273-a18a-00f15af4b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1491224/1834783058.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Loss: 1.7076933725241907, Acc: 0.4988841550010144\n",
      "Model 2\n",
      "Loss: 1.7834693770623968, Acc: 0.4557381483735714\n",
      "Model 3\n",
      "Loss: 1.7421031810701892, Acc: 0.5314803543653209\n",
      "Model 4\n",
      "Loss: 1.7958272489189715, Acc: 0.5457496449584094\n",
      "Model 5\n",
      "Loss: 1.713580052951112, Acc: 0.5374315276932441\n",
      "Model 6\n",
      "Loss: 1.9951718523171607, Acc: 0.43335362142422396\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 7):\n",
    "    # Load the model\n",
    "    epoch = 18 if i == 1 else 20\n",
    "    model_path = f\"models_checkpoints/model_{i}/model_{epoch}_out_of_20.ckpt\"\n",
    "    model = None\n",
    "    \n",
    "    if i == 1:\n",
    "        num_segments = 8\n",
    "        model = GestureClassifier1(input_size = RES_NEXT_OUT * num_segments, num_classes=len(labels_encode_dict))\n",
    "    elif i == 2:\n",
    "        num_segments = 8\n",
    "        model = GestureClassifier2(input_size = RES_NEXT_OUT * num_segments, num_classes=len(labels_encode_dict))\n",
    "    else:\n",
    "        if i in [3, 4]:\n",
    "            num_segments = 8\n",
    "        elif i == 5:\n",
    "            num_segments = 4\n",
    "        elif i == 6:\n",
    "            num_segments = 2\n",
    "        \n",
    "        model = GestureClassifier3_6(input_size = RES_NEXT_OUT * num_segments, num_classes=len(labels_encode_dict))\n",
    "\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "    # Dataset\n",
    "    random = True if i <= 3 else False\n",
    "    dataset = VideoDataset(DATA_ROOT, \"test\", labels_encode_dict, transform=preprocess_transform, n_segments=num_segments, random=random)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Settings\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    res_next.to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    #summary(model, input_size = (RES_NEXT_OUT*num_segments,))\n",
    "\n",
    "    loss, acc = evaluate(model, res_next, loader, criterion, device)\n",
    "    print(f\"Model {i}\")\n",
    "    print(f\"Loss: {loss}, Acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b35a4d-710a-48af-98b2-1480e3aa6f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (Computer Vision venv)",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
