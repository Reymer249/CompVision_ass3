{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b80b68-7a2b-47ad-bd33-ef55c8b70c73",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6962350b-d329-4138-bf99-25368ee45451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef172f79-eb14-48b5-ba20-77647c7f3dc0",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae441cdd-2f5e-4abd-9a24-c94ed92f4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SEGMENTS = 8\n",
    "RES_NEXT_OUT = 2048\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d88c94e2-309c-472c-a5a6-6c2fc99a2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_PATH = \"jester-v1-labels.csv\"\n",
    "with open(LABELS_PATH) as labels_file:\n",
    "    labels = labels_file.readlines()\n",
    "    #labels = [label[:-1] for label in labels]\n",
    "    labels_encode_dict = dict(zip(labels, range(len(labels))))\n",
    "    labels_decode_dict = dict(zip(range(len(labels)), labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599544c-b4cc-49dd-a857-ce64ac5cf6a7",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c43deb4-0184-424f-95f7-1f4645f68bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # define \n",
    "        self.ln1 = nn.Linear(input_size, int(input_size/4))\n",
    "        self.ln2 = nn.Linear(int(input_size/4), int(input_size/8))\n",
    "        self.ln3 = nn.Linear(int(input_size/8), num_classes)\n",
    "\n",
    "        # init\n",
    "        self.initialize_layer(self.ln1)\n",
    "        self.initialize_layer(self.ln2)\n",
    "        self.initialize_layer(self.ln3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.ln1(x))\n",
    "        x = torch.relu(self.ln2(x))\n",
    "        x = self.ln3(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7187acf-32f1-4ac3-90e0-5238ccad5e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 4096]      67,112,960\n",
      "            Linear-2                 [-1, 2048]       8,390,656\n",
      "            Linear-3                   [-1, 27]          55,323\n",
      "================================================================\n",
      "Total params: 75,558,939\n",
      "Trainable params: 75,558,939\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 288.23\n",
      "Estimated Total Size (MB): 288.34\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = GestureClassifier(input_size = RES_NEXT_OUT * NUM_SEGMENTS, num_classes=len(labels_encode_dict))\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "summary(model, input_size = (RES_NEXT_OUT*NUM_SEGMENTS,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0aad96-929a-447e-9a6b-bca52783c0d7",
   "metadata": {},
   "source": [
    "# Frame selection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4b5c3d-8a48-4cd8-94c0-d58d33ee7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "random = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb86dc5-d318-4c51-a999-3cab78ad6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def _select_frames(list_of_frames, num_segments, random):\n",
    "    n = len(list_of_frames)\n",
    "    segment_boundaries = np.linspace(0, n, num_segments + 1, dtype=int)  # Define segment boundaries\n",
    "    if not random:\n",
    "        selected_indices = segment_boundaries[:-1]  # Take the first index of each segment\n",
    "    else:\n",
    "        selected_indices = [np.random.randint(segment_boundaries[i], segment_boundaries[i + 1]) \n",
    "                        for i in range(num_segments)]  # Sample 1 index per segment\n",
    "    selected_frames = [list_of_frames[i] for i in selected_indices]  # Map indices to frames\n",
    "\n",
    "    return selected_frames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (Computer Vision venv)",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
