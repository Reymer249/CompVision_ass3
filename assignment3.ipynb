{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200cc611-9d15-4e1b-83b8-cb363baa4e00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Notes\n",
    "\n",
    "1) Add preprocessing transformation - DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a3b92-6375-4921-b5eb-98356381b4c5",
   "metadata": {},
   "source": [
    "# Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ba1c06-0d04-485b-ba9f-91fb70f78943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd222ba3-8b3a-4e15-9240-bd2b111dae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE = 8\n",
    "NUM_SEGMENTS = 8\n",
    "RES_NEXT_OUT = 2048\n",
    "NUM_EPOCHS = 20\n",
    "CHECKPOINT_FOLDER = os.path.join('models_checkpoints', 'model_2')\n",
    "METRICS_FOLDER = os.path.join('metrics', 'model_2')\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa2cfe0-a555-4c5d-8e37-844120896e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_PATH = \"jester-v1-labels.csv\"\n",
    "TRAIN_LABELS = \"train.csv\"\n",
    "VAL_LABLES = \"val.csv\"\n",
    "TEST_LABELS = \"test.csv\"\n",
    "DATA_ROOT = \"20bn-jester-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42316bf-4004-4e1f-9519-a2be1a84575c",
   "metadata": {},
   "source": [
    "Load the pretrained ResNeXt101_32x8d model to use it for feature extraction \\\n",
    "We load it here as we get the transformation function from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0decc522-93a8-49f4-9bb1-e08a95a0e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = models.ResNeXt101_32X8D_Weights.DEFAULT\n",
    "res_next = models.resnext101_32x8d(weights=weights)\n",
    "res_next.eval()\n",
    "res_next = nn.Sequential(*list(res_next.children())[:-1])\n",
    "# Freeze all layers so that they are not updated during training\n",
    "for param in res_next.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Get the transformations needed for the model\n",
    "preprocess_transform = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1746744-db90-4120-b760-d00b8bb2ee19",
   "metadata": {},
   "source": [
    "# Define the datasets and data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36a268-ad5d-4b16-b54f-ac428d19dd1a",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d839161b-0236-485c-a0d4-3b4aee8d01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABELS_PATH) as labels_file:\n",
    "    labels = labels_file.readlines()\n",
    "    #labels = [label[:-1] for label in labels]\n",
    "    labels_encode_dict = dict(zip(labels, range(len(labels))))\n",
    "    labels_decode_dict = dict(zip(range(len(labels)), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1268574e-4e0d-4078-9cb8-9f4bf61cf038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, label_dict, transform=None, n_segments=None, frame_limit=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with the root directory for the videos,\n",
    "        the split (train/val/test), an optional data transformation,\n",
    "        and an optional label dictionary.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Root directory for videos\n",
    "            split (str): Split to use ('train', 'val', or 'test').\n",
    "            transform (callable, optional): Optional data transformation to apply to the images.\n",
    "            label_dict (dict, optional): Optional dictionary mapping integer labels to class names.\n",
    "        \"\"\"\n",
    "        assert split in ['train', 'val', 'test']\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.label_dict = label_dict\n",
    "        self.frame_limit = frame_limit\n",
    "        self.n_segments = n_segments\n",
    "        self.videos_paths = []\n",
    "        self.labels_num = []\n",
    "        self.labels_str = []\n",
    "\n",
    "        with open(self.split + '.csv') as r:\n",
    "            lines = r.readlines()\n",
    "            for line in lines:\n",
    "                line = line.split(';')\n",
    "                self.videos_paths.append(line[0])\n",
    "                self.labels_num.append(label_dict[line[1]])\n",
    "                self.labels_str.append(line[1])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of images in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of images in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels_num)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _select_frames(list_of_frames, num_segments):\n",
    "        n = len(list_of_frames)\n",
    "        segment_boundaries = np.linspace(0, n, num_segments+1, dtype=int)  # Define segment boundaries\n",
    "        selected_indices = [np.random.randint(segment_boundaries[i], segment_boundaries[i + 1]) \n",
    "                            for i in range(num_segments)]  # Sample 1 index per segment\n",
    "        selected_frames = [list_of_frames[i] for i in selected_indices]  # Map indices to frames\n",
    "    \n",
    "        return selected_frames\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.root_dir, self.videos_paths[idx])\n",
    "        label = self.labels_num[idx]\n",
    "    \n",
    "        # Load all frames in the video\n",
    "        frame_files = sorted([f for f in os.listdir(video_path) if f.endswith(\".jpg\")])\n",
    "        if self.frame_limit:\n",
    "            frame_files = frame_files[:self.frame_limit]\n",
    "\n",
    "        if self.n_segments:\n",
    "            frame_files = self._select_frames(frame_files, self.n_segments)\n",
    "    \n",
    "        frames = []\n",
    "        for frame_file in frame_files:\n",
    "            frame_path = os.path.join(video_path, frame_file)\n",
    "            frame = Image.open(frame_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)  # Apply transform to convert to tensor\n",
    "            else:\n",
    "                frame = transforms.ToTensor()(frame)  # Default conversion if no transform provided\n",
    "            frames.append(frame)\n",
    "    \n",
    "        # Stack frames into a tensor (T x C x H x W)\n",
    "        video_tensor = torch.stack(frames)\n",
    "    \n",
    "        return video_tensor, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222121cc-acbb-43c8-b81a-c22599659a47",
   "metadata": {},
   "source": [
    "## Define datasets and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e391e0c-1bf3-4adc-ac6d-4b070bc4f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations_train = v2.Compose([\n",
    "    v2.RandomApply([v2.ElasticTransform(alpha=50.0, sigma=9.0)], p=0.2),\n",
    "    v2.ColorJitter(\n",
    "        brightness=0.1,\n",
    "        contrast=0.1,\n",
    "        saturation=0.1,\n",
    "        hue=0.1\n",
    "    ),\n",
    "    v2.RandomAdjustSharpness(sharpness_factor=2),\n",
    "    v2.RandomAutocontrast(),\n",
    "    v2.RandomEqualize(),\n",
    "    preprocess_transform\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9685df16-51a5-4a5e-82aa-019ada589c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(DATA_ROOT, \"train\", labels_encode_dict, transform=transformations_train, n_segments=NUM_SEGMENTS)\n",
    "val_dataset = VideoDataset(DATA_ROOT, \"val\", labels_encode_dict, transform=preprocess_transform, n_segments=NUM_SEGMENTS)\n",
    "test_dataset = VideoDataset(DATA_ROOT, \"test\", labels_encode_dict, transform=preprocess_transform, n_segments=NUM_SEGMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00815ae7-a1b2-46ce-adb0-d306d5e6eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d850b029-7d4a-4f0e-8e27-2c4fd1553984",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices = torch.randperm(len(train_dataset)).tolist()[:100]  # Select 100 samples randomly\n",
    "train_small = Subset(train_dataset, subset_indices)\n",
    "subset_indices = torch.randperm(len(val_dataset)).tolist()[:100]  # Select 100 samples randomly\n",
    "val_small = Subset(val_dataset, subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3304da98-e259-40e5-ad48-ebca70a8828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_loader = DataLoader(\n",
    "    train_small,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_small_loader = DataLoader(\n",
    "    val_small,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa84edb-ed3c-44fe-9dd9-225b18395cb7",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55ba61-6147-414b-a8f1-bc22e4efc3fe",
   "metadata": {},
   "source": [
    "ResNeXt output - 2048 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bafb5969-5f72-4086-955a-848c3647a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # define \n",
    "        self.ln1 = nn.Linear(input_size, int(input_size/4))\n",
    "        self.ln2 = nn.Linear(int(input_size/4), int(input_size/8))\n",
    "        self.ln3 = nn.Linear(int(input_size/8), num_classes)\n",
    "\n",
    "        # init\n",
    "        self.initialize_layer(self.ln1)\n",
    "        self.initialize_layer(self.ln2)\n",
    "        self.initialize_layer(self.ln3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.ln1(x))\n",
    "        x = torch.relu(self.ln2(x))\n",
    "        x = self.ln3(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65378d03-efb4-4e07-a13c-be4f1d0ee316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 4096]      67,112,960\n",
      "            Linear-2                 [-1, 2048]       8,390,656\n",
      "            Linear-3                   [-1, 27]          55,323\n",
      "================================================================\n",
      "Total params: 75,558,939\n",
      "Trainable params: 75,558,939\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 288.23\n",
      "Estimated Total Size (MB): 288.34\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = GestureClassifier(input_size = RES_NEXT_OUT * NUM_SEGMENTS, num_classes=len(labels_encode_dict))\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "summary(model, input_size = (RES_NEXT_OUT*NUM_SEGMENTS,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1c52f-171e-432d-b262-650073320882",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a1d2d8b-8b75-4adf-8858-ae618fe75d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_batch(res_next, video_batch):\n",
    "    \"\"\"\n",
    "    Process video frames and extract features using res_next.\n",
    "    Args:\n",
    "        res_next: Pretrained feature extractor (e.g., ResNeXt).\n",
    "        video_batch: Tensor of shape [batch_size, num_frames, 3, 224, 224].\n",
    "\n",
    "    Returns:\n",
    "        Concatenated features for each video: [batch_size, num_frames * feature_dim].\n",
    "    \"\"\"\n",
    "    batch_size, num_frames, c, h, w = video_batch.shape\n",
    "\n",
    "    # Reshape to process frames independently\n",
    "    frames = video_batch.view(batch_size * num_frames, c, h, w)  # [batch_size * num_frames, 3, 224, 224]\n",
    "    \n",
    "    # Extract features for each frame\n",
    "    frame_features = res_next(frames)  # Output shape: [batch_size * num_frames, feature_dim]\n",
    "    \n",
    "    # Reshape back to group frames for each video\n",
    "    frame_features = frame_features.view(batch_size, num_frames, -1)  # [batch_size, num_frames, feature_dim]\n",
    "    \n",
    "    # Concatenate features along the temporal dimension\n",
    "    fused_features = frame_features.view(batch_size, -1)  # [batch_size, num_frames * feature_dim]\n",
    "    \n",
    "    return fused_features\n",
    "\n",
    "\n",
    "def evaluate(model, features_model, eval_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the classifier on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model (CNN): CNN classifier to evaluate.\n",
    "        features_model: CNN to extract features from images. \n",
    "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
    "        criterion (callable): Loss function to use for evaluation.\n",
    "        device (torch.device): Device to use for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss on the test set.\n",
    "        float: Accuracy on the test set.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        for inputs, labels in eval_loader:\n",
    "            # Move inputs and labels to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute the logits and loss\n",
    "            logits = model(process_video_batch(features_model, inputs))\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            num_samples += len(inputs)\n",
    "\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = num_correct / num_samples\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train(model, features_model, train_loader, val_loader, optimizer, criterion, device,\n",
    "          num_epochs, with_train_set_metrics=False):\n",
    "    \"\"\"\n",
    "    Train the classifer on the training set and evaluate it on the validation set every epoch.\n",
    "\n",
    "    Args:\n",
    "    model (CNN): classifier to train.\n",
    "    features_model: CNN to extract features from images. \n",
    "    train_loader (torch.utils.data.DataLoader): Data loader for the training set.\n",
    "    val_loader (torch.utils.data.DataLoader): Data loader for the validation set.\n",
    "    optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
    "    criterion (callable): Loss function to use for training.\n",
    "    device (torch.device): Device to use for training.\n",
    "    num_epochs (int): Number of epochs to train the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Place the model on device\n",
    "    model = model.to(device)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Set model to training mode\n",
    "\n",
    "        with tqdm(total=len(train_loader),\n",
    "                  desc=f'Epoch {epoch +1}/{num_epochs}',\n",
    "                  position=0,\n",
    "                  leave=True) as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                #Move inputs and labels to device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Compute the logits and loss\n",
    "                logits = model(process_video_batch(features_model, inputs))\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "                \n",
    "            avg_loss, accuracy = evaluate(model, features_model, val_loader, criterion, device)\n",
    "            print(\n",
    "                f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}'\n",
    "                )\n",
    "            if with_train_set_metrics:\n",
    "                train_avg_loss, train_accuracy = evaluate(model, features_model, train_loader, criterion, device)\n",
    "                print (\n",
    "                    f'Train set: Average loss = {train_avg_loss:.4f}, Accuracy = {train_accuracy:.4f}'\n",
    "                )\n",
    "                losses.append((train_avg_loss, avg_loss))\n",
    "                accuracies.append((train_accuracy, accuracy))\n",
    "            else:\n",
    "                losses.append(avg_loss)\n",
    "                accuracies.append(accuracy)\n",
    "            with open(os.path.join(METRICS_FOLDER, 'losses.pkl'), 'wb') as f:\n",
    "                pickle.dump(losses, f)\n",
    "            with open(os.path.join(METRICS_FOLDER, 'accuracies.pkl'), 'wb') as f:\n",
    "                pickle.dump(accuracies, f)\n",
    "            torch.save(\n",
    "                {\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict':optimizer.state_dict()\n",
    "                },\n",
    "                os.path.join(CHECKPOINT_FOLDER, f'model_{epoch+1}_out_of_{num_epochs}.ckpt')\n",
    "            )\n",
    "\n",
    "        # plt.clf()  # Clear the current figure\n",
    "        # plt.plot(losses[:, 0], label='Training Loss')\n",
    "        # plt.plot(losses[:, 1], label='Validation Loss')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.ylabel('Loss')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        # plt.pause(0.1)  # Pause to update the plot\n",
    "        torch.save(\n",
    "            {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict()\n",
    "            }, \n",
    "            os.path.join(CHECKPOINT_FOLDER, 'model.ckpt')\n",
    "        )\n",
    "\n",
    "    return losses, accuracies\n",
    "\n",
    "def test(model, features_model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Get predictions for the test set.\n",
    "\n",
    "    Args:\n",
    "        model (CNN): classifier to evaluate.\n",
    "        features_model: CNN to extract features from images. \n",
    "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
    "        device (torch.device): Device to use for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss on the test set.\n",
    "        float: Accuracy on the test set.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            # Move inputs and labels to device\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            logits = model(process_video_batch(features_model, inputs))\n",
    "\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            preds = list(zip(labels, predictions.tolist()))\n",
    "            all_preds.extend(preds)\n",
    "    return all_preds\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = num_correct / num_samples\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0084f2bb-a5ff-4e9e-913c-5c437703d66d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd507e57-28e1-497a-b0a6-c820f0491b8b",
   "metadata": {},
   "source": [
    "Define the parameters of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a970ea9-250c-4c48-bd00-6c6545eec931",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GestureClassifier(input_size = RES_NEXT_OUT * NUM_SEGMENTS, num_classes=len(labels_encode_dict))\n",
    "model.to(device)\n",
    "res_next.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d672ee2-4ff9-4f2e-8721-7aaa795f4872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|███████████████████| 25/25 [00:06<00:00,  4.63it/s, loss=0.666]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss = 0.0323, Accuracy = 0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|███████████████████| 25/25 [00:20<00:00,  1.23it/s, loss=0.666]\n",
      "Epoch 2/20: 100%|████████████████████| 25/25 [00:06<00:00,  4.64it/s, loss=0.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss = 0.0315, Accuracy = 0.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|████████████████████| 25/25 [00:19<00:00,  1.25it/s, loss=0.17]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_small_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_small_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 135\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, features_model, train_loader, val_loader, optimizer, criterion, device, num_epochs, with_train_set_metrics)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(METRICS_FOLDER, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracies.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    134\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(accuracies, f)\n\u001b[0;32m--> 135\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINT_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_out_of_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# plt.clf()  # Clear the current figure\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# plt.plot(losses[:, 0], label='Training Loss')\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# plt.plot(losses[:, 1], label='Validation Loss')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# plt.pause(0.1)  # Pause to update the plot\u001b[39;00m\n\u001b[1;32m    151\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m    152\u001b[0m     {\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CHECKPOINT_FOLDER, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    157\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/computer_vision/lib/python3.10/site-packages/torch/serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/computer_vision/lib/python3.10/site-packages/torch/serialization.py:690\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, res_next, train_loader, val_loader, optimizer, criterion, device, NUM_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (Computer Vision venv)",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
